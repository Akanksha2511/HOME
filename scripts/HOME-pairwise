# -*- coding: utf-8 -*-
"""
Created on Wed Apr 26 19:05:38 2017

@author: akanksha
"""

import time
import argparse 
import numpy as np
import os
import glob
import subprocess 
import sys
import shutil
import functools
import pandas as pd
import multiprocessing
import multiprocessing.pool
from HOME import HOME_functions as ho
start_time1 = time.time()  
class NoDaemonProcess(multiprocessing.Process):
    
    def _get_daemon(self):
        return False
    def _set_daemon(self, value):
        pass
    daemon = property(_get_daemon, _set_daemon)

class MyPool(multiprocessing.pool.Pool):
    Process = NoDaemonProcess

np.set_printoptions(threshold=np.inf,suppress=True,linewidth=np.inf,precision=3)

parser = argparse.ArgumentParser(description='HOME -- HISTOGRAM Of METHYLATION',formatter_class=argparse.ArgumentDefaultsHelpFormatter)

parser.add_argument('-b','--pathsample1', help='path of the first sample methylome', required=True, nargs='+')
parser.add_argument('-a','--pathsample2',  help='path of the second sample methylome', required=True, nargs='+')
parser.add_argument('-t','--type', help='type of class', choices=["CG","CHG","CHH","CHN","CNN"],required=True, type=str)
parser.add_argument('-o','--outputpath', help='path where the DMRs will be saved', required=True)
parser.add_argument('-fn','--outputfilename', help='filename of the where the output DMRs will be saved', required=False, type=str,default="HOME_DMRs.txt")
parser.add_argument('-sc','--scorecutoff',  help='min score required to cluster the DMR',choices=np.round(np.linspace(0,1,20,endpoint=False),decimals=2), required=False, type=float,default=0.1)
parser.add_argument('-p','--pruncutoff', help='prunning cutoff for boundaries', required=False,choices=np.round(np.arange(0,0.5,0.1),decimals=1), type=float,default=0.1)
parser.add_argument('-ml','--minlength', help='minimum length of DMRs to reported', required=False, type=int,default=50)
parser.add_argument('-ncb','--numcb',  help='number of Cs required between DMRs to keep them seperate', required=False, type=int,default=5)
parser.add_argument('-md','--mergedist',  help='distance between DMRs to merge', required=False, type=int,default=500)
parser.add_argument('-sin','--singlechrom',  help='parallel for single chromosomes',action='store_true',default=False)
parser.add_argument('-npp','--numprocess', help='number of parallel processes for all chromosome', required=False, type=int,default=8)

parser.add_argument('-mc','--minc', help='minimum number of C in a DMR to reported', required=False, type=int,default=5)
parser.add_argument('-d','--delta', help='minimum average difference in methylation required', required=False, type=float,default=0.1)
parser.add_argument('-prn','--prunningC', help='number of Cs required for prunning', required=False, type=int,default=3)

o=parser.parse_args()
input_file1=o.pathsample1
input_file2=o.pathsample2
try:
    cwd = os.getcwd()
    if not os.path.exists((o.outputpath+'/temp_HOME')):
        os.makedirs(o.outputpath+'/temp_HOME')
    for j in xrange(len(input_file1)):
        
        if not os.path.exists((o.outputpath+'/temp_HOME'+'/sample1_rep'+str(j+1))):
            os.makedirs(o.outputpath+'/temp_HOME'+'/sample1_rep'+str(j+1))
            os.chdir(o.outputpath+'/temp_HOME'+'/sample1_rep'+str(j+1))
            if input_file1[j].endswith('.gz'):
                com='zcat'+' '+input_file1[j]+'''| awk '{print $0 >> $1".tsv"}' '''
            else:
                com='''awk '{print $0 >> $1".tsv"}' ''' + input_file1[j]
            subprocess.call(com, shell=True)
              
            input_file1[j] = os.getcwd()
            os.chdir(cwd)
    for j in xrange(len(input_file2)): 
        if not os.path.exists((o.outputpath+'/temp_HOME'+'/sample2_rep'+str(j+1))):
            os.makedirs(o.outputpath+'/temp_HOME'+'/sample2_rep'+str(j+1))
            os.chdir(o.outputpath+'/temp_HOME'+'/sample2_rep'+str(j+1))
            if input_file2[j].endswith('.gz'):
                com='zcat'+' '+input_file2[j]+'''| awk '{print $0 >> $1".tsv"}' '''
            else:
                com='''awk '{print $0 >> $1".tsv"}' ''' + input_file2[j]
            subprocess.call(com, shell=True)
            
            input_file2[j] = os.getcwd()
            os.chdir(cwd)
    
    s=[ os.path.splitext(os.path.basename(x))[0] for x in glob.glob(input_file1[0]+'/*.tsv')]
    os.chdir(cwd)
    if not os.path.exists((o.outputpath+'/temp_HOME'+'/chunks')):
        os.makedirs(o.outputpath+'/temp_HOME'+'/chunks')
    if not os.path.exists((o.outputpath+'/HOME_DMRs')):
        os.makedirs(o.outputpath+'/HOME_DMRs')    
    if not os.path.exists((o.outputpath+'/HOME_DMRs_filtered')):
        os.makedirs(o.outputpath+'/HOME_DMRs_filtered')
   
    sc=o.scorecutoff
    prn=o.prunningC
    tr=o.pruncutoff
    minlen=o.minlength
    classes=o.type
    
    dis_thres=o.mergedist
    ncb=o.numcb
    mc=o.minc
    d=o.delta
    sin=o.singlechrom
    npp=o.numprocess
    if sin==True:
        
        nop=npp
        npp=1
    if sin==False: 
        nop=1
    fn=o.outputfilename
    "handle any number of replicates as long as it is 2+ in all groups but cannot handle 1 replicate in 1 group and multiple in the other"
    if (len(input_file1)==1 and len(input_file2)>1) or (len(input_file2)==1 and len(input_file1)>1):
        sys.exit('error: cannot handle 1 replicate in 1 group and more than 1 in other')
    print"Preparing the DMRs from HOME....." 
    print "GOOD LUCK !"
    pd.options.mode.chained_assignment = None
    
    
    
    merge = functools.partial(pd.merge, how='inner', on=['chr','pos',"strand","type"])
except:
    pass    
def main(c):
  
   
    val=[]
    d_rep={}
    for j in xrange(len(input_file1)):
        c1=["chr", "pos", "strand", "type", "mc_cont"+"_rep"+str(j+1), "h_cont"+"_rep"+str(j+1)]
        
        d_rep[j]=pd.read_table(input_file1[j]+'/'+c+'.tsv',header=None, names=c1)
        
        if classes=="CG":
            d_rep[j]=d_rep[j].loc[d_rep[j]['type'].str[:2] == "CG"]
        elif classes=="CHG":
            
            d_rep[j]=d_rep[j].loc[(d_rep[j]['type'].str[1:2]!="G") & (d_rep[j]['type'].str[2:3]=="G")]
        elif classes=="CHH":
            
            d_rep[j]=d_rep[j].loc[(d_rep[j]['type'].str[1:2]!="G") & (d_rep[j]['type'].str[2:3]!="G")] 
        elif classes=="CHN":
            
            d_rep[j]=d_rep[j].loc[(d_rep[j]['type'].str[1:2]!="G") ] 
        else:
            d_rep[j]=d_rep[j]
        
    val.append(functools.reduce(merge, d_rep.values()))    
    
    d_rep={}
    for j in xrange(len(input_file2)):
        c1=["chr", "pos", "strand", "type", "mc_case"+"_rep"+str(j+1), "h_case"+"_rep"+str(j+1)]
        d_rep[j]=pd.read_table(input_file2[j]+'/'+c+'.tsv',header=None, names=c1)
        if classes=="CG":
            d_rep[j]=d_rep[j].loc[d_rep[j]['type'].str[:2] == "CG"]
        elif classes=="CHG":
            
            d_rep[j]=d_rep[j].loc[(d_rep[j]['type'].str[1:2]!="G") & (d_rep[j]['type'].str[2:3]=="G")]
        elif classes=="CHH":
            
            d_rep[j]=d_rep[j].loc[(d_rep[j]['type'].str[1:2]!="G") & (d_rep[j]['type'].str[2:3]!="G")] 
        elif classes=="CHN":
            
            d_rep[j]=d_rep[j].loc[(d_rep[j]['type'].str[1:2]!="G") ] 
        else:
            d_rep[j]=d_rep[j]
        
    val.append(functools.reduce(merge, d_rep.values()))
    
    df=functools.reduce(merge, val)
    
    df=df.sort_values(['pos'])
    del(val)
    
    df1=ho.format_allc(df, classes)
    
    del(df) 
    if len(input_file1)>1 and len(input_file2)>1:
        CHUNKSIZE = int(len(df1)/nop)
        CHUNKSIZE_list=[CHUNKSIZE]*nop 
        extra=(len(df1)%nop)
        
        if extra> 0:
            CHUNKSIZE_list[-1]=CHUNKSIZE_list[-1]+extra
        df_chunk=ho.chunker1(df1,CHUNKSIZE_list)
        ttc=0
        df_path=[]
        for i in df_chunk:
            ttc=ttc+1
            i.to_csv(o.outputpath+'/temp_HOME'+'/chunks'+"/{ttc}.txt".format(ttc=ttc),header=True, index=False,sep='\t')    
            df_path.append(o.outputpath+'/temp_HOME'+'/chunks'+"/{ttc}.txt".format(ttc=ttc))
        pool = multiprocessing.Pool(processes=nop)
        process = [pool.apply_async(ho.process_frame_withR, args=(dd,)) for dd in df_path]
        pool.close()
        
        pool.join()
        
        pool = multiprocessing.Pool(processes=nop)
        process = [pool.apply_async(ho.pval_format_withrep, args=(dd,)) for dd in df_path]
        pool.close()
        
        pool.join()
        output = [p.get() for p in process]
       
        df3=pd.concat(output, ignore_index=True,axis=0)
            
        smooth_exp_val=ho.smoothing(*df3.exp_val) 
        
        df3['smooth_val']=(smooth_exp_val-min(smooth_exp_val))/(max(smooth_exp_val)-min(smooth_exp_val))
        
    else:
       df3=ho.pval_cal_withoutrep(df1)
   
    if classes=="CG":
        input_file_path=os.getcwd()+'/training_data/training_data_CG.txt'
        
        model_path=os.getcwd()+'/saved_model/saved_model_CG.pickle'
        
        k=ho.norm_slidingwin_predict_CG(df3,input_file_path,model_path)
    elif classes=="CHG" or classes=="CHH" or classes=="CHN":
   
        input_file_path=os.getcwd()+'/training_data/training_data_nonCG.txt'
        
        model_path=os.getcwd()+'/saved_model/saved_model_nonCG.pickle'
        CHUNKSIZE = int(len(df3)/nop)
        df_chunk=ho.chunker(df3,CHUNKSIZE)
        pool = multiprocessing.Pool(processes=nop)
    
        process = [pool.apply_async(ho.norm_slidingwin_predict_nonCG, args=(dd,input_file_path,model_path)) for dd in df_chunk]
    
        pool.close()
        pool.join()
        output = [p.get() for p in process]
        k=pd.concat(output, ignore_index=True,axis=0)
        
    
    if classes=="CG" :
        len_cutoff=10
        tr=o.pruncutoff
        dmrs=ho.clustandtrim_CG(k,df3,sc,tr,dis_thres,ncb,prn,len_cutoff)
    if classes=="CHG" or classes=="CHH" or classes=="CHN":
        len_cutoff=2
        
        k["meth_case"]=df3.meth_case
        k["meth_cont"]=df3.meth_cont
        k["meth_diff"]=df3.meth_diff
        k["h_cont"]=df3.h_cont
        k["h_case"]=df3.h_case
        del(df3)
        CHUNKSIZE = int(len(k)/nop)
        CHUNKSIZE_list=[CHUNKSIZE]*nop 
        extra=(len(k)%nop)
        
        if extra> 0:
            CHUNKSIZE_list[-1]=CHUNKSIZE_list[-1]+extra
        df_chunk=ho.chunker1(k,CHUNKSIZE_list)
    
        pool = multiprocessing.Pool(processes=nop)
        process = [pool.apply_async(ho.clustandtrim_nonCG1, args=(dd,sc)) for dd in df_chunk]
        pool.close()
        pool.join()
        output = [p.get() for p in process]
        
        pre_dmr=pd.concat(output, ignore_index=True,axis=0)
        
        df_split=ho.splitlist(k,pre_dmr,nop,dis_thres)
        del(pre_dmr,k)
       
        pool = multiprocessing.Pool(processes=nop)
        process = [pool.apply_async(ho.clustandtrim_nonCG2, args=(ddd,dd,dis_thres,ncb,len_cutoff)) for dd,ddd in df_split]
        pool.close()
        pool.join()
        output = [p.get() for p in process]
        
        dmrs=pd.concat(output, ignore_index=True,axis=0)
       
    if classes=="CHG" or classes=="CHH" or classes=="CHN":
        dmrs_filtered=ho.filterdmr_nonCG(dmrs,minlen,mc) 
    if classes=="CG" :
        dmrs_filtered=ho.filterdmr(dmrs,minlen,mc,d)
   
    if len(dmrs)> 1: 
        dmr_final=pd.concat([df1.chr[0:len(dmrs)],dmrs],axis=1)
        dmr_final['chr'] = dmr_final['chr'].astype(str)
        dmr_final.to_csv(o.outputpath+'/HOME_DMRs'+"/"+"HOME_DMR_{c}.txt".format(c=c),header=True, index=False,sep='\t')
        print "DMRs for {c} done".format(c=c)
     
    if len(dmrs_filtered)> 1: 
        dmr_final=pd.concat([df1.chr[0:len(dmrs_filtered)],dmrs_filtered],axis=1)
        dmr_final['chr'] = dmr_final['chr'].astype(str)
        dmr_final.to_csv(o.outputpath+'/HOME_DMRs_filtered'+"/"+"HOME_DMR_{c}.txt".format(c=c),header=True, index=False,sep='\t')    
        return 
if __name__ == "__main__":
 if npp==1:  
   try:   
    for dx in s:
       main(dx)
    shutil.rmtree(o.outputpath+'/temp_HOME', ignore_errors=True)
        
    print "Congratulations the DMRs are ready"   
   except :
          shutil.rmtree(o.outputpath+'/temp_HOME', ignore_errors=True)
          print "error in the files"   
       
 if npp>1:
      try: 
       
        pool1= MyPool(processes=npp)
        process1 = [pool1.apply_async(main, args=(dx,)) for dx in s]
        
        pool1.close()
        pool1.join()
       
        shutil.rmtree(o.outputpath+'/temp_HOME', ignore_errors=True)
        
        print "Congratulations the DMRs are ready"
        
      except :
          shutil.rmtree(o.outputpath+'/temp_HOME', ignore_errors=True)
          print "error in the files"